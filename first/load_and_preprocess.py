import pandas as pd
import json
import chinese_province_city_area_mapper.mappers as mapper
import glob
import warnings
from tqdm import tqdm
import pyarrow.parquet as pq
import time
import os

def parse_purchase_history(record):
    """è§£æJSONæ ¼å¼çš„è´­ä¹°è®°å½•"""
    try:
        data = json.loads(record.replace("'", '"'))
        return pd.Series({
            'avg_price': data.get('average_price', 0),
            'category': data.get('category', 'unknown'),
            'items_count': len(data.get('items', []))
        })
    except:
        return pd.Series({'avg_price': 0, 'category': 'unknown', 'items_count': 0})

def load_data(file_pattern):
    """é«˜æ•ˆåŠ è½½åˆå¹¶å¤šä¸ªCSVæ–‡ä»¶"""
    chunks = []
    for file in glob.glob(file_pattern):
        for chunk in pd.read_csv(file, chunksize=10000):
            # è§£æå…³é”®å­—æ®µ
            chunk[['avg_price', 'category', 'items_count']] = chunk['purchase_history'].apply(parse_purchase_history)
            
            # åœ°å€è§£æ
            # ä»ä¸­æ–‡åœ°å€ä¸­æå–çœå¸‚ä¿¡æ¯
            # ä»mapper.province_country_mapperçš„keyä¸­è·å–çœä¿¡æ¯åˆ—è¡¨ï¼ˆåŒ…å«å¸‚ï¼Œçœï¼Œè‡ªæ²»åŒºï¼‰
            province_mapper = mapper.province_country_mapper
            province_list = list(province_mapper.keys())
            # ç­›é€‰åªä»¥å¸‚ï¼Œçœï¼Œè‡ªæ²»åŒºï¼Œç‰¹åˆ«è¡Œæ”¿åŒºç»“å°¾çš„çœä¿¡æ¯
            province_list = [i for i in province_list if i.endswith(('å¸‚', 'çœ', 'è‡ªæ²»åŒº', 'ç‰¹åˆ«è¡Œæ”¿åŒº'))]
            # è§£æchunk["chinese_address"]åˆ—
            chunk['province'] = chunk['chinese_address'].apply(lambda x: [i for i in province_list if i in x])
            chunk['province'] = chunk['province'].apply(lambda x: x[0] if len(x) > 0 else 'unknown') # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°çœä»½ï¼Œåˆ™æ ‡è®°ä¸º'unknown'
            # print(chunk['province'])
            
            chunks.append(chunk)
    return pd.concat(chunks)

def load_data_test(file_pattern):
    """é«˜æ•ˆåŠ è½½åˆå¹¶å¤šä¸ªCSVæ–‡ä»¶"""
    chunks = []
    
    # è·å–æ–‡ä»¶åˆ—è¡¨
    files = glob.glob(file_pattern)
    
    # ç¬¬ä¸€å±‚è¿›åº¦æ¡ï¼šæ–‡ä»¶å¤„ç†è¿›åº¦
    with tqdm(files, desc="ğŸ“‚ Processing files", unit="file") as file_pbar:
        for file in file_pbar:
            # è®¾ç½®å½“å‰æ–‡ä»¶çš„è¿›åº¦æè¿°
            file_pbar.set_postfix(file=file.split("\\")[-1][:10])  # æ˜¾ç¤ºæ–‡ä»¶åï¼ˆå–å10å­—ç¬¦ï¼‰
            
            # ç¬¬äºŒå±‚è¿›åº¦æ¡ï¼šå—å¤„ç†è¿›åº¦
            chunk_iter = pd.read_csv(file, chunksize=1000000)
            with tqdm(desc="ğŸ“¦ Processing chunks", unit="chunk", leave=False) as chunk_pbar:
                for chunk in chunk_iter:
                    # è§£æå…³é”®å­—æ®µ
                    chunk[['avg_price', 'category', 'items_count']] = chunk['purchase_history'].apply(parse_purchase_history)
                    
                    # åœ°å€è§£æ
                    # ä»ä¸­æ–‡åœ°å€ä¸­æå–çœå¸‚ä¿¡æ¯
                    province_mapper = mapper.province_country_mapper
                    province_list = [p for p in province_mapper.keys() 
                                   if p.endswith(('å¸‚', 'çœ', 'è‡ªæ²»åŒº', 'ç‰¹åˆ«è¡Œæ”¿åŒº'))]
                    # è§£æchunk["chinese_address"]åˆ—
                    chunk['province'] = chunk['chinese_address'].apply(
                        lambda x: next((p for p in province_list if p in x), 'unknown'))
                    
                    chunks.append(chunk)
                    chunk_pbar.update(1)  # æ›´æ–°å—è¿›åº¦æ¡
                    chunk_pbar.set_postfix(current_size=f"{len(chunks)*10000:,} rows") 
    return pd.concat(chunks)

def load_parquet_data(valid_files, if_file_pattern=False):
    """Parquetæ–‡ä»¶è¯»å–"""
    files = glob.glob(valid_files) if if_file_pattern else valid_files
    print(f"è¯»å– {len(files)} ä¸ªæ–‡ä»¶")
    all_dfs = []
    
    # è¿›åº¦æ¡é…ç½®
    file_progress = tqdm(files, desc="æ–‡ä»¶è¿›åº¦", unit="file", 
                       bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]")
    
    for file in file_progress:
        try:
            # è·å–æ–‡ä»¶å…ƒæ•°æ®
            parquet_file = pq.ParquetFile(file)
            total_rows = parquet_file.metadata.num_rows
            file_size = os.path.getsize(file) / 1024**2  # MB
            
            # åˆå§‹åŒ–æ–‡ä»¶è¿›åº¦æ¡
            file_progress.set_postfix(
                file=os.path.basename(file)[:10],
                size=f"{file_size:.1f}MB",
                rows=f"{total_rows//10000}ä¸‡è¡Œ"
            )
            
            # åˆ›å»ºè¯»å–è¿›åº¦æ¡
            read_progress = tqdm(
                total=total_rows,
                desc=f"è¯»å– {os.path.basename(file)[:10]}",
                unit="row",
                leave=False,
                bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"
            )
            
            # åˆ†æ‰¹æ¬¡è¯»å–ï¼ˆè‡ªåŠ¨å†…å­˜ç®¡ç†ï¼‰
            batches = []
            for batch in parquet_file.iter_batches(batch_size=1250000):
                # è®°å½•è¯»å–å¼€å§‹æ—¶é—´
                start_time = time.time()
                
                # è½¬æ¢batchä¸ºpandas DataFrame
                df = batch.to_pandas()
                
                # è§£æå­—æ®µ
                df[['avg_price', 'category', 'items_count']] = df['purchase_history'].apply(parse_purchase_history)
                
                # åœ°å€è§£æ
                # ä»mapper.province_country_mapperçš„keyä¸­è·å–çœä¿¡æ¯åˆ—è¡¨ï¼ˆåŒ…å«å¸‚ï¼Œçœï¼Œè‡ªæ²»åŒºï¼‰
                province_mapper = mapper.province_country_mapper
                province_list = list(province_mapper.keys())
                # ç­›é€‰åªä»¥å¸‚ï¼Œçœï¼Œè‡ªæ²»åŒºï¼Œç‰¹åˆ«è¡Œæ”¿åŒºç»“å°¾çš„çœä¿¡æ¯
                province_list = [i for i in province_list if i.endswith(('å¸‚', 'çœ', 'è‡ªæ²»åŒº', 'ç‰¹åˆ«è¡Œæ”¿åŒº'))]
                # è§£æchunk["chinese_address"]åˆ—
                df['province'] = 'unknown'
                for province in province_list:
                    mask = df['chinese_address'].str.contains(province, na=False)
                    df.loc[mask, 'province'] = province
                
                batches.append(df)
                
                # æ›´æ–°è¿›åº¦æ¡
                batch_rows = len(df)
                time_cost = time.time() - start_time
                read_progress.update(batch_rows)
                read_progress.set_postfix(
                    speed=f"{batch_rows/time_cost:.0f} rows/s",
                    mem=f"{df.memory_usage(deep=True).sum()/1024**2:.1f}MB"
                )
            
            # åˆå¹¶å½“å‰æ–‡ä»¶æ•°æ®
            file_df = pd.concat(batches, ignore_index=True)
            all_dfs.append(file_df)
            read_progress.close()
            
        except Exception as e:
            print(f"\n æ–‡ä»¶ {file} è¯»å–å¤±è´¥: {str(e)}")
            continue
    
    # åˆå¹¶æ‰€æœ‰æ–‡ä»¶æ•°æ®
    return pd.concat(all_dfs, ignore_index=True) if all_dfs else pd.DataFrame()

def main():
    # ä½¿ç”¨ç¤ºä¾‹
    df = load_data("./data/*.csv")  # æ”¯æŒé€šé…ç¬¦åŒ¹é…å¤šä¸ªæ–‡ä»¶
    
if __name__ == "__main__":
    main()