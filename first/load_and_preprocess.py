import pandas as pd
import json
import chinese_province_city_area_mapper.mappers as mapper
import glob
import warnings
from tqdm import tqdm

def parse_purchase_history(record):
    """è§£æJSONæ ¼å¼çš„è´­ä¹°è®°å½•"""
    try:
        data = json.loads(record.replace("'", '"'))
        return pd.Series({
            'avg_price': data.get('average_price', 0),
            'category': data.get('category', 'unknown'),
            'items_count': len(data.get('items', []))
        })
    except:
        return pd.Series({'avg_price': 0, 'category': 'unknown', 'items_count': 0})

def load_data(file_pattern):
    """é«˜æ•ˆåŠ è½½åˆå¹¶å¤šä¸ªCSVæ–‡ä»¶"""
    chunks = []
    for file in glob.glob(file_pattern):
        for chunk in pd.read_csv(file, chunksize=10000):
            # è§£æå…³é”®å­—æ®µ
            chunk[['avg_price', 'category', 'items_count']] = chunk['purchase_history'].apply(parse_purchase_history)
            
            # åœ°å€è§£æ
            # ä»ä¸­æ–‡åœ°å€ä¸­æå–çœå¸‚ä¿¡æ¯
            # ä»mapper.province_country_mapperçš„keyä¸­è·å–çœä¿¡æ¯åˆ—è¡¨ï¼ˆåŒ…å«å¸‚ï¼Œçœï¼Œè‡ªæ²»åŒºï¼‰
            province_mapper = mapper.province_country_mapper
            province_list = list(province_mapper.keys())
            # ç­›é€‰åªä»¥å¸‚ï¼Œçœï¼Œè‡ªæ²»åŒºï¼Œç‰¹åˆ«è¡Œæ”¿åŒºç»“å°¾çš„çœä¿¡æ¯
            province_list = [i for i in province_list if i.endswith(('å¸‚', 'çœ', 'è‡ªæ²»åŒº', 'ç‰¹åˆ«è¡Œæ”¿åŒº'))]
            # è§£æchunk["chinese_address"]åˆ—
            chunk['province'] = chunk['chinese_address'].apply(lambda x: [i for i in province_list if i in x])
            chunk['province'] = chunk['province'].apply(lambda x: x[0] if len(x) > 0 else 'unknown') # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°çœä»½ï¼Œåˆ™æ ‡è®°ä¸º'unknown'
            # print(chunk['province'])
            
            chunks.append(chunk)
    return pd.concat(chunks)

def load_data_test(file_pattern):
    """é«˜æ•ˆåŠ è½½åˆå¹¶å¤šä¸ªCSVæ–‡ä»¶"""
    chunks = []
    
    # è·å–æ–‡ä»¶åˆ—è¡¨
    files = glob.glob(file_pattern)
    
    # ç¬¬ä¸€å±‚è¿›åº¦æ¡ï¼šæ–‡ä»¶å¤„ç†è¿›åº¦
    with tqdm(files, desc="ğŸ“‚ Processing files", unit="file") as file_pbar:
        for file in file_pbar:
            # è®¾ç½®å½“å‰æ–‡ä»¶çš„è¿›åº¦æè¿°
            file_pbar.set_postfix(file=file.split("\\")[-1][:10])  # æ˜¾ç¤ºæ–‡ä»¶åï¼ˆå–å10å­—ç¬¦ï¼‰
            
            # ç¬¬äºŒå±‚è¿›åº¦æ¡ï¼šå—å¤„ç†è¿›åº¦
            chunk_iter = pd.read_csv(file, chunksize=1000000)
            with tqdm(desc="ğŸ“¦ Processing chunks", unit="chunk", leave=False) as chunk_pbar:
                for chunk in chunk_iter:
                    # è§£æå…³é”®å­—æ®µ
                    chunk[['avg_price', 'category', 'items_count']] = chunk['purchase_history'].apply(parse_purchase_history)
                    
                    # åœ°å€è§£æ
                    # ä»ä¸­æ–‡åœ°å€ä¸­æå–çœå¸‚ä¿¡æ¯
                    province_mapper = mapper.province_country_mapper
                    province_list = [p for p in province_mapper.keys() 
                                   if p.endswith(('å¸‚', 'çœ', 'è‡ªæ²»åŒº', 'ç‰¹åˆ«è¡Œæ”¿åŒº'))]
                    # è§£æchunk["chinese_address"]åˆ—
                    chunk['province'] = chunk['chinese_address'].apply(
                        lambda x: next((p for p in province_list if p in x), 'unknown'))
                    
                    chunks.append(chunk)
                    chunk_pbar.update(1)  # æ›´æ–°å—è¿›åº¦æ¡
                    chunk_pbar.set_postfix(current_size=f"{len(chunks)*10000:,} rows") 
    return pd.concat(chunks)
    
def main():
    # ä½¿ç”¨ç¤ºä¾‹
    df = load_data("./data/*.csv")  # æ”¯æŒé€šé…ç¬¦åŒ¹é…å¤šä¸ªæ–‡ä»¶
    
if __name__ == "__main__":
    main()